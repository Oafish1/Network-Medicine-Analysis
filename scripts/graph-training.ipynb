{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd \"Compound GRN ENC Analysis/scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import sklearn.svm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "# Params\n",
    "DATA_FOLDER = os.path.join(os.path.abspath(''), '../../data')\n",
    "RESULTS_FOLDER = os.path.join(os.path.abspath(''), '../results')\n",
    "PLOTS_FOLDER = os.path.join(os.path.abspath(''), '../plots')\n",
    "\n",
    "# Style\n",
    "sns.set_theme(context='talk', style='white', palette='Accent')\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "# matplotlib.rcParams['font.family'] = 'Helvetica'  # NOTE: Make sure to download Helvetica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Consider weight scaling for disease and ctl individually (maybe?  Depends on methodology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert from Gerstein to project format\n",
    "# save_dir = os.path.join(DATA_FOLDER, 'merged_GRNs_v2', 'Subclass', 'ctrl')\n",
    "# read_dir = os.path.join(save_dir, 'original')\n",
    "# fnames = os.listdir(read_dir)\n",
    "# for fname in fnames:\n",
    "#     data = pd.read_csv(os.path.join(read_dir, fname), delimiter='\\t', low_memory=False)\n",
    "#     data = data.rename(columns={'TG': 'target', 'edgeWeight': 'importance'})[['TF', 'target', 'importance']]\n",
    "#     data = data.loc[data['importance'] > data['importance'].quantile(.9)]  # Filter to top 10%\n",
    "#     data.to_csv(os.path.join(save_dir, fname[:-7] + 'merged.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cohort : Disease : Delimiter\n",
    "CMC: SCZ : tsv\n",
    "UCLA_ASD: ASD : csv\n",
    "Urban_DLPFC: BPD, SCZ : tsv\n",
    "Subclass: ASD, BPD, SCZ : csv\n",
    "\"\"\"\n",
    "data_sources = (\n",
    "    ('CMC', 'SCZ', '\\t'),\n",
    "    ('UCLA_ASD', 'ASD', ','),\n",
    "    ('Urban_DLPFC', 'BPD', '\\t'),\n",
    "    ('Urban_DLPFC', 'SCZ', '\\t'),  # Removed for low sample size\n",
    "    ('Subclass', 'ASD', ','),\n",
    "    ('Subclass', 'BPD', ','),\n",
    "    ('Subclass', 'SCZ', ','),\n",
    "    ('Coregulation', 'ASD', ','),\n",
    "    ('Coregulation', 'BPD', ','),\n",
    "    ('Coregulation', 'SCZ', ','),\n",
    ")\n",
    "modules = [None, 1, 2]\n",
    "group, disease, delimiter = data_sources[0]\n",
    "use_ctl = True\n",
    "use_modules = modules[0]\n",
    "omit_distal_links = True\n",
    "\n",
    "# SCRIPT (Indent everything following this)\n",
    "# for source, use_modules in product(data_sources, modules):\n",
    "#     group, disease, delimiter = source\n",
    "#     use_ctl = group != 'Subclass'\n",
    "if group == 'Coregulation': use_ctl = False; omit_distal_links = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AD, BPD, and SCZ labels\n",
    "gene_dir = os.path.join(DATA_FOLDER, 'new_labels')\n",
    "gene_fnames = [fname for fname in os.listdir(gene_dir) if fname.endswith('.txt')]\n",
    "gene_lists = {'.'.join(fname.split('.')[:-1]): np.loadtxt(os.path.join(gene_dir, fname), dtype=str) for fname in gene_fnames}\n",
    "gene_lists['BPD'] = gene_lists.pop('BD')\n",
    "\n",
    "# Get ASD labels\n",
    "sfari = pd.read_csv(os.path.join(DATA_FOLDER, 'sfari/SFARI-Gene_genes_01-16-2024release_03-21-2024export.csv'))\n",
    "gene_score_threshold = -1\n",
    "sfari = sfari.loc[sfari['gene-score'] > gene_score_threshold]  # Threshold by score\n",
    "gene_lists['ASD'] = sfari['gene-symbol'].to_numpy()\n",
    "\n",
    "# Get module genes\n",
    "def get_module_genes(group, disease, ct):\n",
    "    gene_annotations = pd.read_csv(os.path.join(DATA_FOLDER, 'modules', get_modules_fname(use_modules=use_modules), f'{ct}_{group}_{disease}.txt'), index_col=False, delimiter=',')\n",
    "    positive_genes = gene_annotations.loc[gene_annotations['label']=='positive', 'gene'].to_list()\n",
    "    negative_genes = gene_annotations.loc[gene_annotations['label']=='negative', 'gene'].to_list()\n",
    "    return positive_genes, negative_genes\n",
    "\n",
    "# Get files for contrast\n",
    "def get_grn_fnames(group, disease, use_ctl=True):\n",
    "    # Calculate directories\n",
    "    base_dir = os.path.join(DATA_FOLDER, 'merged_GRNs_v2', group)\n",
    "    disease_folder = os.path.join(base_dir, disease)\n",
    "    if use_ctl:\n",
    "        control_folder = os.path.join(base_dir, 'ctrl')\n",
    "        grn_fnames = np.sort(list(set(os.listdir(disease_folder)).intersection(set(os.listdir(control_folder)))))\n",
    "    else: grn_fnames = np.sort(os.listdir(disease_folder)) \n",
    "\n",
    "    # Return\n",
    "    ret = ()\n",
    "    ret += (base_dir, disease_folder)\n",
    "    if use_ctl: ret += (control_folder,)\n",
    "    ret += (grn_fnames,)\n",
    "    # base_dir, disease_folder, control_folder, grn_fnames\n",
    "    return ret\n",
    "if use_ctl: base_dir, disease_folder, control_folder, grn_fnames = get_grn_fnames(group, disease, use_ctl=use_ctl)\n",
    "else: base_dir, disease_folder, grn_fnames = get_grn_fnames(group, disease, use_ctl=use_ctl)\n",
    "\n",
    "# Get fname suffix\n",
    "def get_modules_fname(use_modules, **kwargs): return f'model{use_modules}' if use_modules is not None else ''\n",
    "def get_fname_suffix(**kwargs):\n",
    "    suffixes = [get_modules_fname(**kwargs)]\n",
    "    suffixes = [s for s in suffixes if len(s) > 0]\n",
    "    if len(suffixes) == 0: return ''\n",
    "    return f'_{\"_\".join(suffixes)}'\n",
    "\n",
    "# Get cell-type based on fname\n",
    "def get_cell_type(fname):\n",
    "    split_fname = fname.split('_')\n",
    "    if split_fname[-1] == 'coreg.matrix.txt':\n",
    "        return split_fname[1]\n",
    "    return '_'.join(split_fname[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass major pairing\n",
    "inhibitory = ['Lamp5', 'Pvalb', 'Sncg', 'Sst','Sst.Chodl', 'Lamp5.Lhx6', 'Vip', 'Pax6', 'Chandelier']\n",
    "excitatory = ['L2.3.IT', 'L4.IT', 'L5.IT', 'L5.ET', 'L5.6.NP', 'L6b', 'L6.IT', 'L6.CT', 'L6.IT.Car3']\n",
    "major = ['astro', 'endo', 'excitatory', 'inhibitory', 'micro', 'oligo', 'opc', 'vlmc']\n",
    "ct_conversion = {**{ct: 'inhibitory' for ct in inhibitory}, **{ct: 'excitatory' for ct in excitatory}, **{ct: ct for ct in major}}\n",
    "# Disease pairings\n",
    "group_conversion = {'ASD': 'UCLA_ASD', 'BPD': 'Urban_DLPFC', 'SCZ': 'CMC'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Classification\n",
    "        self.net = gnn.Sequential('x, edge_index, edge_weight', [\n",
    "            # Pure weighted GCN\n",
    "            (gnn.GCNConv(self.input_dim, self.embedding_dim), 'x, edge_index, edge_weight -> x'),\n",
    "            nn.BatchNorm1d(self.embedding_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout(.5),\n",
    "\n",
    "            (gnn.GCNConv(-1, self.embedding_dim // 2), 'x, edge_index, edge_weight -> x'),\n",
    "            nn.BatchNorm1d(self.embedding_dim // 2),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout(.5),\n",
    "\n",
    "            (gnn.GCNConv(-1, 2), 'x, edge_index, edge_weight -> x'),\n",
    "        ])\n",
    "\n",
    "        # Embedding weights\n",
    "        self.embedding_net = nn.Sequential(\n",
    "            nn.Linear(2 * self.embedding_dim, 1),\n",
    "            # nn.Linear(self.embedding_dim, 1),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        # return torch.matmul(self.net[0].lin.weight.T, self.net[3].lin.weight.T)\n",
    "        return self.net[0].lin.weight.T\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        return self.net(x, edge_index, edge_weight)\n",
    "\n",
    "    def predict_adjacency(self, idx=None):\n",
    "        \"\"\"Embedding loss function\"\"\"\n",
    "        # Get embeddings\n",
    "        embeddings = self.get_embeddings()\n",
    "\n",
    "        # Compare embeddings\n",
    "        predicted_adjacency_matrix = torch.zeros(embeddings.shape[0] if idx is None else len(idx), embeddings.shape[0])\n",
    "        for iter_num, i in enumerate(range(embeddings.shape[0]) if idx is None else idx):\n",
    "            x = torch.concat((embeddings[[i]].expand(embeddings.shape[0], -1), embeddings), dim=1)\n",
    "            predicted_adjacency_matrix[iter_num] = self.embedding_net(x).squeeze(dim=1)\n",
    "        predicted_adjacency_matrix.fill_diagonal_(0)  # Don't count self-comparisons\n",
    "\n",
    "        return predicted_adjacency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_coreg_network(df):\n",
    "    # NOTE: There is a gene named TF, which is why this melt chronology is used\n",
    "    df = df.melt(id_vars='Gene', var_name='target', value_name='importance').rename(columns={'Gene': 'TF'})\n",
    "    df = df.loc[df['TF'] != df['target']]  # Remove self-loops\n",
    "    # df = df.loc[df['importance'] >= df['importance'].quantile(.99)]  # Filter to high values\n",
    "    df = df.loc[df['importance'] > 0]  # Remove zero values\n",
    "    df = pd.concat((df, df.rename(columns={'TF': 'target', 'target': 'TF'})), axis=0)  # Make undirected\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def pd_to_tg(df, positive_genes, negative_genes=None, omit_distal_links=True):\n",
    "    \"\"\"Convert pandas dataframe to torch geometric dataset\"\"\"\n",
    "    # Read\n",
    "    df = df.rename(columns=dict(target='TG', importance='weight'))\n",
    "    if omit_distal_links:\n",
    "        # Only two types, distal and proximal\n",
    "        df = df.loc[df['link'] == 'proximal']\n",
    "\n",
    "    # Extract information\n",
    "    unique = np.unique(list(df['TF'])+list(df['TG']))\n",
    "    x = np.eye(unique.shape[0])\n",
    "    node_dict = {k: v for v, k in enumerate(unique)}\n",
    "    node_dict_rev = {v: k for k, v in node_dict.items()}\n",
    "    from_edges = df['TF'].apply(lambda x: node_dict[x]).to_numpy()\n",
    "    to_edges = df['TG'].apply(lambda x: node_dict[x]).to_numpy()\n",
    "    edge_index = np.stack((from_edges, to_edges), axis=0)\n",
    "    edge_weight = df['weight'].to_numpy()\n",
    "\n",
    "    # Get y data\n",
    "    # -1 is unknown, 0 is no disease, 1 is disease\n",
    "    y = np.zeros(x.shape[0])-1  # Assign genes as unknown\n",
    "    y[[node_dict_rev[i] in positive_genes for i in range(x.shape[0])]] = 1  # Annotate positive genes\n",
    "    if negative_genes is not None:\n",
    "        y[[node_dict_rev[i] in negative_genes for i in range(x.shape[0])]] = 0  # Annotate negative genes\n",
    "    else:\n",
    "        y[np.argwhere(y!=1).flatten()] = 0  # Mark all others as negative\n",
    "\n",
    "    # Cast to tensors\n",
    "    x = torch.tensor(x).float()\n",
    "    edge_index = torch.tensor(edge_index).long()\n",
    "    edge_weight = torch.tensor(edge_weight).float()\n",
    "    if 'Cell Type' in df: edge_attr = torch.tensor(edge_attr).float()\n",
    "    y = torch.tensor(y).long()\n",
    "\n",
    "    # Torch data\n",
    "    data = torch_geometric.data.Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_weight=edge_weight,\n",
    "        y=y)\n",
    "\n",
    "    # Additional attributes\n",
    "    data.known_mask = y!=-1\n",
    "    data.adj_mat = torch.zeros(data.x.shape[0], data.x.shape[0])\n",
    "    for (tf, tg), w in zip(data.edge_index.T, data.edge_weight):\n",
    "        data.adj_mat[tf, tg] = w\n",
    "    data.node_dict = node_dict\n",
    "    data.disease = df['disease'].to_numpy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Parameters\n",
    "epochs = 101\n",
    "epoch_cli = 20\n",
    "folds = 5\n",
    "embedding_loss_weight = 1e3\n",
    "\n",
    "# Run files\n",
    "stats = defaultdict(lambda: [])\n",
    "for fname in grn_fnames:\n",
    "    # Parameters\n",
    "    cell_type = get_cell_type(fname)\n",
    "\n",
    "    # Load graph\n",
    "    disease_graph = pd.read_csv(os.path.join(disease_folder, fname), index_col=False, delimiter=delimiter)\n",
    "    if group == 'Coregulation': disease_graph = format_coreg_network(disease_graph)\n",
    "    disease_graph['disease'] = True\n",
    "    if use_ctl:\n",
    "        control_graph = pd.read_csv(os.path.join(control_folder, fname), index_col=False, delimiter=delimiter)\n",
    "        control_graph['disease'] = False\n",
    "        combined_graph = pd.concat((disease_graph, control_graph), axis=0)\n",
    "    else:\n",
    "        combined_graph = disease_graph\n",
    "\n",
    "    # Procure gene lists\n",
    "    if group in ('Subclass', 'Coregulation'):\n",
    "        # Use major group for module labels\n",
    "        major_group = group_conversion[disease]\n",
    "        # Get major cell type\n",
    "        major_cell_type = ct_conversion[cell_type] if cell_type in ct_conversion else None\n",
    "    else:\n",
    "        major_group = group\n",
    "        major_cell_type = cell_type\n",
    "    if not use_modules: positive_genes, negative_genes = gene_lists[disease], None\n",
    "    else:\n",
    "        if major_cell_type is None: continue  # Skip if no major cell type\n",
    "        positive_genes, negative_genes = get_module_genes(major_group, disease, major_cell_type)\n",
    "    graph = pd_to_tg(combined_graph, positive_genes, negative_genes=negative_genes, omit_distal_links=omit_distal_links)\n",
    "\n",
    "    # Run folds\n",
    "    folds_idx = torch.randperm(graph.x.shape[0]).split(int(np.ceil(graph.x.shape[0]/folds)))\n",
    "    for fold in range(folds):\n",
    "        # Get idx\n",
    "        val_fold = fold\n",
    "        train_folds = [i for i in range(folds) if i != val_fold]\n",
    "        graph.train_idx, graph.val_idx = torch.concat([folds_idx[i] for i in train_folds]), folds_idx[val_fold]\n",
    "        graph.train_mask = torch.zeros(graph.x.shape[0], dtype=torch.bool); graph.train_mask[graph.train_idx] = True\n",
    "        graph.val_mask = torch.zeros(graph.x.shape[0], dtype=torch.bool); graph.val_mask[graph.val_idx] = True\n",
    "        graph.train_mask *= graph.known_mask; graph.val_mask *= graph.known_mask\n",
    "        graph.train_idx = graph.train_mask.argwhere().flatten(); graph.val_idx = graph.val_mask.argwhere().flatten()\n",
    "\n",
    "        # Create model\n",
    "        model = GCN(graph.x.shape[1]).train()\n",
    "        # Calculate weights\n",
    "        # weight = torch.unique(graph.y[graph.train_mask], return_counts=True)[1]\n",
    "        # weight = weight.sum() / weight; weight = weight / weight.sum()\n",
    "        # Get helpers\n",
    "        classification_criterion = nn.CrossEntropyLoss()  # weight=weight\n",
    "        embedding_criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=.96)\n",
    "\n",
    "        # CLI\n",
    "        print(f'{cell_type} - Fold {fold:02d}')\n",
    "        for epoch in range(epochs):\n",
    "            # Run for disease and control\n",
    "            disease_z = model(graph.x, graph.edge_index[:, graph.disease], graph.edge_weight[graph.disease])\n",
    "            if use_ctl: control_z = model(graph.x, graph.edge_index[:, ~graph.disease], graph.edge_weight[~graph.disease])\n",
    "\n",
    "            # Get losses\n",
    "            idx = graph.train_idx\n",
    "            sub_idx = np.random.choice(graph.train_idx, len(graph.train_idx) // 10, replace=False)  # TODO: Add as a tunable hyperparameter\n",
    "            disease_classification_loss = classification_criterion(disease_z[idx], graph.y[idx])\n",
    "            if use_ctl: control_classification_loss = classification_criterion(control_z[idx], graph.y[idx])\n",
    "            embedding_loss = embedding_loss_weight * embedding_criterion(model.predict_adjacency(sub_idx), graph.adj_mat[sub_idx])  # Maybe replace adj_mat with disease or control?\n",
    "            loss = disease_classification_loss\n",
    "            if use_ctl: loss += control_classification_loss\n",
    "            model.net[0].lin.weight.requires_grad = False\n",
    "            loss.backward()\n",
    "            model.net[0].lin.weight.requires_grad = True\n",
    "            embedding_loss.backward()\n",
    "\n",
    "            # Step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "            # CLI\n",
    "            if epoch % epoch_cli == 0:\n",
    "                # Training\n",
    "                print(' - '.join([\n",
    "                    f'epoch {epoch:04d}',\n",
    "                    f'dl: {disease_classification_loss.detach().item():.3f}',\n",
    "                    f'ccl: {(control_classification_loss.detach().item() if use_ctl else 0):.3f}',\n",
    "                    f'el: {embedding_loss.detach().item():.3f}',\n",
    "                    f'total: {loss.detach().item():.3f}',\n",
    "                ]), end=' _ ')\n",
    "\n",
    "                # Validation\n",
    "                with torch.no_grad():\n",
    "                    idx = graph.val_idx\n",
    "                    disease_classification_loss = classification_criterion(disease_z[idx], graph.y[idx])\n",
    "                    if use_ctl: control_classification_loss = classification_criterion(control_z[idx], graph.y[idx])\n",
    "                    embedding_loss = embedding_loss_weight * embedding_criterion(model.predict_adjacency(idx), graph.adj_mat[idx])\n",
    "                    loss = disease_classification_loss\n",
    "                    if use_ctl: loss += control_classification_loss\n",
    "                print(' - '.join([\n",
    "                    f'val dl: {disease_classification_loss.detach().item():.3f}',\n",
    "                    f'val ccl: {(control_classification_loss.detach().item() if use_ctl else 0):.3f}',\n",
    "                    f'val el: {embedding_loss.detach().item():.3f}',\n",
    "                    f'val total: {loss.detach().item():.3f}',\n",
    "                ]))\n",
    "\n",
    "        # Finish training\n",
    "        model.eval()\n",
    "\n",
    "        # Record\n",
    "        stats['Cell Type'].append(cell_type)\n",
    "        stats['Fold'].append(fold)\n",
    "\n",
    "        # Evaluation\n",
    "        disease_z = model(graph.x, graph.edge_index[:, graph.disease], graph.edge_weight[graph.disease]).detach()\n",
    "        if use_ctl: control_z = model(graph.x, graph.edge_index[:, ~graph.disease], graph.edge_weight[~graph.disease]).detach()\n",
    "        pred_prob = F.softmax(disease_z, dim=1)[:, 1]\n",
    "\n",
    "        # Training\n",
    "        auprc = sklearn.metrics.average_precision_score(graph.y[graph.train_mask], pred_prob[graph.train_mask])\n",
    "        # CLI\n",
    "        print(' - '.join([\n",
    "            f'auprc: {auprc:.3f}',\n",
    "        ]), end=' _ ')\n",
    "        # Record\n",
    "        stats['AUPRC'].append(auprc)\n",
    "\n",
    "        # Validation\n",
    "        auprc = sklearn.metrics.average_precision_score(graph.y[graph.val_mask], pred_prob[graph.val_mask])\n",
    "        # CLI\n",
    "        print(' - '.join([\n",
    "            f'val auprc: {auprc:.3f}',\n",
    "        ]), end=' _ ')\n",
    "        # Record\n",
    "        stats['Validation AUPRC'].append(auprc)\n",
    "\n",
    "        # Baselines\n",
    "        print(' - '.join([\n",
    "            f'base auprc: {graph.y[graph.known_mask].float().mean():.3f}',\n",
    "        ]))\n",
    "\n",
    "        # Save predictions\n",
    "        pred = F.softmax(disease_z, dim=1)[:, 1]\n",
    "        node_dict_rev = {v: k for k, v in graph.node_dict.items()}\n",
    "        node_labels = [node_dict_rev[i] for i in range(len(graph.node_dict))]\n",
    "        df = pd.DataFrame({'gene': node_labels, 'score': pred.numpy(), 'validation': graph.val_mask.numpy(), 'label': graph.y.numpy()})\n",
    "        df.to_csv(os.path.join(RESULTS_FOLDER, f'{group}_{disease}_{cell_type}_{fold}_prioritized_genes{get_fname_suffix(use_modules=use_modules)}.csv'), index=False)\n",
    "\n",
    "        # Formatting\n",
    "        print()\n",
    "\n",
    "    # CLI\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results in cell-type files\n",
    "for fname in grn_fnames:\n",
    "    df = None\n",
    "    for fold in range(folds):\n",
    "        # Read file\n",
    "        try: df_join = pd.read_csv(os.path.join(RESULTS_FOLDER, f'{group}_{disease}_{get_cell_type(fname)}_{fold}_prioritized_genes{get_fname_suffix(use_modules=use_modules)}.csv'))\n",
    "        except: continue\n",
    "        df_join = df_join.set_index('gene')\n",
    "        df_join = df_join.rename(columns=lambda s: (s + f'_{fold}') if s not in ('gene', 'label') else s)\n",
    "\n",
    "        # Escape if first\n",
    "        if df is None: df = df_join; continue\n",
    "\n",
    "        # Concat\n",
    "        df_join = df_join.drop(columns='label')\n",
    "        df = df.join(df_join)\n",
    "\n",
    "    # Add mean and std\n",
    "    scores = df[[f'score_{fold}' for fold in range(folds)]]\n",
    "    df['mean'] = scores.mean(axis=1)\n",
    "    df['std'] = scores.std(axis=1)\n",
    "\n",
    "    # Order\n",
    "    df = df[['label', 'mean', 'std'] + [f'score_{fold}' for fold in range(folds)] + [f'validation_{fold}' for fold in range(folds)]]\n",
    "\n",
    "    # Save to file\n",
    "    df.to_csv(os.path.join(RESULTS_FOLDER, f'{group}_{disease}_{get_cell_type(fname)}_prioritized_genes{get_fname_suffix(use_modules=use_modules)}.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Save stats\n",
    "df = pd.DataFrame(stats)\n",
    "fname = os.path.join(RESULTS_FOLDER, f'{group}_{disease}_performance{get_fname_suffix(use_modules=use_modules)}.csv')\n",
    "df.to_csv(fname)  # Save stats\n",
    "# df = pd.read_csv(fname)  # Load stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
